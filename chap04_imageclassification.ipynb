{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "777f0193",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d88bfb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a614e29e",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24f7b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipurl = 'https://pytorch.tips/bee-zip'\n",
    "with urlopen(zipurl) as zipresp:\n",
    "    with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "        zfile.extractall('./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626ce9c8",
   "metadata": {},
   "source": [
    "Note:\n",
    "- Data augmentations to make models more robust to variations in size and position\n",
    "    - RandomResizedCrop to 224x224 (standard input for many pretrained CNNs)\n",
    "    - RandomHorizontalFlip (eg. if cat facing left or right)\n",
    "- [ToTensor() docs](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html) also does\n",
    "    - scale pixel values from range [0,255] to [0.0,1.0]\n",
    "    - rearranges dimensions from (height, width, channels) to (channels,height,width) - format expected by PyTorch\n",
    "- Normalize \n",
    "    - applies $\\frac{(channel-mean)}{std}$ for each channel\n",
    "\n",
    "\n",
    "Note:\n",
    "- data augment only for training, **NOT** for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32ee86f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([     # Chain multiple transformations\n",
    "    transforms.RandomResizedCrop(224),      # Randomly crops image and resize to 224x224 \n",
    "    transforms.RandomHorizontalFlip(),      # Randomly flip image horizontally (left-right) with default probability 0.5\n",
    "    transforms.ToTensor(),                  # Converts PIL image or numpy array to PyTorch tensor\n",
    "    transforms.Normalize(\n",
    "        [0.485, 0.456, 0.406],              # ImageNet means for 3 channels (Red, Green, Blue)\n",
    "        [0.229, 0.224, 0.225]               # ImageNet std dev for each channel\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7940ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transforms = transforms.Compose([       # Chain multiple transformations\n",
    "    transforms.Resize(256),                 # Resize to 256x256\n",
    "    transforms.CenterCrop(224),             # Crop in center 224x224\n",
    "    transforms.ToTensor(),                  # Converts PIL image or numpy array to PyTorch tensor\n",
    "    transforms.Normalize(\n",
    "        [0.485, 0.456, 0.406],              # ImageNet means for 3 channels (Red, Green, Blue)\n",
    "        [0.229, 0.224, 0.225]               # ImageNet std dev for each channel\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be9b76bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(\n",
    "    root='data/hymenoptera_data/train',\n",
    "    transform=train_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3f5bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = datasets.ImageFolder(\n",
    "    root='data/hymenoptera_data/val',\n",
    "    transform=val_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "815ee53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,   # batch of 4 at time\n",
    "    shuffle=True,   # shuffle\n",
    "    num_workers=4   # configure for 4 CPU processors in parallel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f8d9d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,   # batch of 4 at time\n",
    "    shuffle=True,   # shuffle\n",
    "    num_workers=4   # configure for 4 CPU processors in parallel\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f565dc7b",
   "metadata": {},
   "source": [
    "# Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bce7b9",
   "metadata": {},
   "source": [
    "ResetNet18 model \n",
    "- pretrained with ImageNet data \n",
    "- designed to detect 1000 classes\n",
    "    - in our case we only need 2 classes (bees and ants)\n",
    "    - modify final layer to detect 2 classes instead of 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa02d32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonlau/.pyenv/versions/3.11.11/envs/deeplearn/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/jasonlau/.pyenv/versions/3.11.11/envs/deeplearn/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/jasonlau/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:01<00:00, 30.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Downloads model to ~/.cache/torch/hub/checkpoints/resnet18-xxx.pth\n",
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1507ecc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9876d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the number of featues before final layer\n",
    "num_ftrs = model.fc.in_features\n",
    "# change final layer by setting directly to fully connected layer with 2 outputs\n",
    "model.fc = nn.Linear(num_ftrs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1d15a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=2, bias=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c54fde",
   "metadata": {},
   "source": [
    "going to \n",
    "- use the pretrained model as starting point\n",
    "- fine-tune its parameters with new data\n",
    "    - since we replaced the final linear layer, it's parameters are now randomly initialized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28b2d13",
   "metadata": {},
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e38edbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ca2d529",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a33ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # Define or loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(  # Define our optimizer algorithm\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    momentum=0.9  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb5e37c",
   "metadata": {},
   "source": [
    "using a PyTorch scheduler to adjust the learning rate of our SGD optimizer after several epochs\n",
    "- will help our NN adjust weights more precisely as training goes on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "694bfbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_lr_scheduler = StepLR( # Use a Learning Rate scheduler\n",
    "    optimizer,\n",
    "    step_size=7,\n",
    "    gamma=0.1       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a0bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=25\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()                       # Training loop\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = models(inputs)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()/inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)/inputs.size(0)\n",
    "    \n",
    "    exp_lr_scheduler.step()         # Schedule the learning rate for next epoch of training\n",
    "\n",
    "    train_epoch_loss = running_loss / len(train_loader)\n",
    "    train_epoch_acc = running_corrects / len(train_loader)\n",
    "\n",
    "    model.eval()                    # Validation loop\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)    \n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        loss = criterion(outputs, labels)     \n",
    "\n",
    "        running_loss += loss.item()/inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)/inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = running_corrects.double() / len(val_loader)\n",
    "\n",
    "    print(f'Train: Loss:{train_epoch_loss:.4f} Acc:{train_epoch_acc:4f} Val: Loss:{epoch_loss:.4f} Acc:{epoch_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dae6e4",
   "metadata": {},
   "source": [
    "# Testing and Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df853fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):                    # define a new function to plot images from our tensor images\n",
    "    inp = inp.numpy().transpose((1,2,0))        # switch from C x H x W to H x W x C\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean                      # undo the normalizations during transforms to properly view images\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd653943",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, classes = next(iter(val_loader))            # Grab a batch of images from our validation dataset\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "class_names = val_dataset.classes\n",
    "\n",
    "outputs = model(inputs.to(device))                  # Perform classification using our fine-tuned ResNet18\n",
    "_, preds = torch.max(outputs, 1)                    # Take the \"winning\" class\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in preds])  # Display the input images and their predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308ad30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./resnet18.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
